content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#main-content") %>%   ##抓指定的資料
html_text                         ##文字資料
}
#載入套件
library(dplyr)
library(jiebaR)
library(jiebaRD)
library(wordcloud2)
library(text2vec)
library(stringr)
library(tidytext)
for(i in 7987:8105){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#main-content") %>%   ##抓指定的資料
html_text                         ##文字資料
}
library(rvest)
for(i in 7987:8105){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#main-content") %>%   ##抓指定的資料
html_text                         ##文字資料
}
url
#-------------------------------------------------------------------------------------
content <- c()   ##抓全部連結裡面的內文
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:8105){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#main-content") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
View(content)
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
library(rvest)
library(tm)
library(jiebaR)
library(Matrix)
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
View(d.corpus)
View(content)
View(content)
content <- c()
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:8105){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#main-content") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
##抓全部連結裡面的內文
content <- c()
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:7988){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("p") %>%   ##抓指定的資料
html_text                         ##文字資料
}
##抓全部連結裡面的內文
content <- c()
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:7988){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#p") %>%   ##抓指定的資料
html_text                         ##文字資料
}
View(d.corpus)
url
content <- c()
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:7988){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content
### 開始清洗
content <- as.list(content)    ##轉成list
View(content)
links <- read_html('https://www.ptt.cc/bbs/LoL/index10004.html') %>%
html_nodes('div.title a') %>%     ##抓網頁裡面每個連結的尾巴
html_attrs %>%
as.character     ##把list轉乘char
links
##抓全部連結裡面的內文
content <- c()
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 7987:7988){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[1] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
View(content)
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, i)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x)  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, length(x))  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x(i))  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
##抓全部連結裡面的內文
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x(i))  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x[i])  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
View(content)
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
View(d.corpus)
### 開始斷詞
mixseg = worker()
jieba_tokenizer = function(d){ # 寫function來處理斷詞
unlist(segment(d[[1]], mixseg))
}
seg <- lapply(d.corpus, jieba_tokenizer) # 對每一個文本(在這裡是每一個網頁內文)執行斷詞函數
n <- length(seg) # n為文本數 之後會一直用到
View(seg)
count_token = function(d){ # 寫function來把清單轉為dataframe
as.data.frame(table(d))
}
tokens = lapply(seg, count_token) # lapply對list中每一個文本做出來的斷詞們轉為dataframe
TDM = tokens[[1]] # 設置一個初始TDM來方便Merge
for(id in 2:n){ # 用迴圈把list裡的所有dataframe merge起來
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', 1:id) # 這裡不一樣是因為我不是讀本機檔
}
TDM[is.na(TDM)] <- 0 # 缺漏值補 0
head(TDM)
View(TDM)
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum) # 等等在算詞頻時會用到
idfCal <- function(word_doc){ # idf 計算的函數 總文本數/出現該詞的文本數
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal) # 對每個 Row 算 idf (有幾Col/幾個不是0)
doc.tfidf <- TDM
# 把剛剛算的tf重複擺在矩陣的Row 有幾個詞擺幾個Row
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
# 把剛剛算的idf擺在矩陣中，每個詞彙有自己獨立的idf
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
# 帥氣的把全部一起算 (每個欄位都算到 (該文本出現該詞彙的次數 / 該文本的詞彙數，也就是TF) * 已經算好的idf)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)]) # tfidf 依照Row加總
delID = which(stopLine == 0) # 找到第幾個詞的RowSum是0
head(doc.tfidf[delID,1]) # 這些就是氾濫字眼 (TF-IDF=0 也就是中央空調的部分log(1) = 0)
TDM = TDM[-delID,] # 不要氾濫字眼就拿掉
doc.tfidf = doc.tfidf[-delID,] # 拿掉
### 找重要的關鍵字
TopWords = data.frame()
for( id in 1:n ){
dayMax = order(doc.tfidf[,id+1], decreasing = TRUE) # 找到每一個文本 按照tf-idf大小排序
showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1])) # 轉置這個dataframe並且取出前5高的
TopWords = rbind(TopWords, showResult) # 跟其他Dataframe合併
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
View(TopWords)
View(TopWords)
View(tokens)
View(tempY)
View(tempX)
View(TDM)
wordcloud2(TDM)
#載入套件
library(dplyr)
library(jiebaR)
library(jiebaRD)
library(wordcloud2)
library(text2vec)
library(stringr)
library(tidytext)
library(rvest)
#讀文檔
setwd("C:/Users/jeff6/Desktop/Github/alan/Week2-4")
data <- readChar("CUG.txt",720000)
str(data)
#开始分词
wk = worker()
seg_MK=wk[data]
#注意：这里要转成list格式
tokens=list(seg_MK)
class(tokens)
# 构造词库（对文中分词结果进行汇总）
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
#删除一个字的词
vocab=vocab[which(nchar(vocab$term)>1),]
#删除出现小于5次的词
vocab = prune_vocabulary(vocab, term_count_min = 5L)
#查看最高频的词
tail(vocab,20)
vectorizer = vocab_vectorizer(vocab)
# 考虑词的前后5个词
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)
#设置词向量是4维的
#glove算法：https://nlp.stanford.edu/projects/glove/
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#构造“唐僧+徒弟”向量
relation = word_vectors["唐僧", , drop = FALSE] +
word_vectors["徒弟", , drop = FALSE]
#计算相关性，查看相关性最高的词
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 20)
library(rvest)
library(tm)
library(jiebaR)
library(Matrix)
##抓全部連結裡面的內文
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x[i])  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
View(d.corpus)
#开始分词
wk = worker()
seg_MK=wk[d.corpus]
#注意：这里要转成list格式
tokens=list(seg_MK)
class(tokens)
# 构造词库（对文中分词结果进行汇总）
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
#删除一个字的词
vocab=vocab[which(nchar(vocab$term)>1),]
#删除出现小于5次的词
vocab = prune_vocabulary(vocab, term_count_min = 5L)
#查看最高频的词
tail(vocab,20)
vectorizer = vocab_vectorizer(vocab)
# 考虑词的前后5个词
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)
#设置词向量是4维的
#glove算法：https://nlp.stanford.edu/projects/glove/
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#构造“唐僧+徒弟”向量
relation = word_vectors["唐僧", , drop = FALSE] +
word_vectors["徒弟", , drop = FALSE]
#计算相关性，查看相关性最高的词
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 20)
#載入套件
library(dplyr)
library(jiebaR)
library(jiebaRD)
library(wordcloud2)
library(text2vec)
library(stringr)
library(tidytext)
library(rvest)
wk = worker()
seg_MK=wk[d.corpus]
#注意：这里要转成list格式
tokens=list(seg_MK)
class(tokens)
# 构造词库（对文中分词结果进行汇总）
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
#删除一个字的词
vocab=vocab[which(nchar(vocab$term)>1),]
#删除出现小于5次的词
vocab = prune_vocabulary(vocab, term_count_min = 5L)
#查看最高频的词
tail(vocab,20)
vectorizer = vocab_vectorizer(vocab)
# 考虑词的前后5个词
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)
#设置词向量是4维的
#glove算法：https://nlp.stanford.edu/projects/glove/
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#构造“唐僧+徒弟”向量
relation = word_vectors["唐僧", , drop = FALSE] +
word_vectors["徒弟", , drop = FALSE]
#计算相关性，查看相关性最高的词
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 20)
wk = worker()
seg_MK=wk[d.corpus]
#注意：这里要转成list格式
tokens=list(seg_MK)
class(tokens)
# 构造词库（对文中分词结果进行汇总）
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
#删除一个字的词
vocab=vocab[which(nchar(vocab$term)>1),]
#删除出现小于5次的词
vocab = prune_vocabulary(vocab, term_count_min = 5L)
#查看最高频的词
tail(vocab,20)
vectorizer = vocab_vectorizer(vocab)
# 考虑词的前后5个词
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)
#设置词向量是4维的
#glove算法：https://nlp.stanford.edu/projects/glove/
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#构造“唐僧+徒弟”向量
relation = word_vectors["唐僧", , drop = FALSE] +
word_vectors["徒弟", , drop = FALSE]
#计算相关性，查看相关性最高的词
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 20)
#載入套件
library(dplyr)
library(jiebaR)
library(jiebaRD)
library(wordcloud2)
library(text2vec)
library(stringr)
library(tidytext)
library(rvest)
library(tm)
library(Matrix)
##抓全部連結裡面的內文
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x[i])  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
### 開始清洗
content <- as.list(content)    ##轉成list
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
View(content)
View(d.corpus)
### 開始斷詞
mixseg = worker()
jieba_tokenizer = function(d){ # 寫function來處理斷詞
unlist(segment(d[[1]], mixseg))
}
seg <- lapply(d.corpus, jieba_tokenizer) # 對每一個文本(在這裡是每一個網頁內文)執行斷詞函數
n <- length(seg) # n為文本數 之後會一直用到
n
View(seg)
View(mixseg)
View(d.corpus)
View(content)
View(d.corpus)
d.corpus <- data.frame(d.corpus)
d.corpus <- as.data.frame(d.corpus)
d.corpus <- as.list(d.corpus)
View(d.corpus)
content <- c()
x <- c(7987:7988)
front <- "https://tw.hjwzw.com/Book/Read/1889,57"
for(i in 1:length(x)){    ##抓links裡面1到最後一個數值 長度的迴圈
url <- paste0(front, x[i])  ##把連結的尾巴加上前面的網址存到url
print(url)
content[i] <- read_html(url) %>%    ##讀url內的文字資料丟到content
html_nodes("#AllySite+ div") %>%   ##抓指定的資料
html_text                         ##文字資料
}
content <- as.list(content)    ##轉成list
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
d.corpus <- as.list(d.corpus)
View(d.corpus)
d.corpus <- as.list(d.corpus)
d.corpus <- as.list(d.corpus)
d.corpus[1]
d.corpus[2]
